{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2760aeb955c43209a670fc078cf9d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d336beccf91940c7a3602a146b7cbeb2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99c694a6654247f4881e546a9d3a6b17",
              "IPY_MODEL_48af9dd7dfd84fb8a9312baf69184f33"
            ]
          }
        },
        "d336beccf91940c7a3602a146b7cbeb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99c694a6654247f4881e546a9d3a6b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c5cb0421cb546c49ff0d9cc3072ef59",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca4e9cd872ff4ff591845ea3223bcefb"
          }
        },
        "48af9dd7dfd84fb8a9312baf69184f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ddc43de903534da4a0f217955fbc53b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:01&lt;00:00, 177MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e8a351f314d4ea69a0a683467ba1a28"
          }
        },
        "4c5cb0421cb546c49ff0d9cc3072ef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca4e9cd872ff4ff591845ea3223bcefb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddc43de903534da4a0f217955fbc53b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e8a351f314d4ea69a0a683467ba1a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_tcTWji0cPG"
      },
      "source": [
        "# Mask R-CNN\r\n",
        "\r\n",
        "This is a Pytorch implimentation of Mask R-CNN that follows [this tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). This is useful for any kind of custom dataset with which you would like to perform image segmentation. The code below allows you to easily perform transfer learning using the model pre-trained on COCO train2017 provided by pytorch.\r\n",
        "\r\n",
        "For my personal usecase, I had small dataset of 10,000 images aggregated using the reddit api wrapper, [praw](https://github.com/praw-dev/praw). I labelled 800 of them, created this entire notebook, and was making accurate out of sample predictions within one sitting. The Dataset class below takes project files (json) generated by the [VGG image annotator](https://www.robots.ox.ac.uk/~vgg/software/via/). For the image datset you can tweak the load_data() method as needed in order to load in your images of choice. Other than that, this notebook is plug and play and will allow you to quickly get up and running with Mask R-CNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkoJvot_OEkM"
      },
      "source": [
        "import os\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "from PIL import Image, ImageDraw, ImageOps, ImageFile\r\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2iclDfLsRM6"
      },
      "source": [
        "if torch.cuda.is_available():\r\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igup16shYol-"
      },
      "source": [
        "root = '/content/drive/MyDrive/Data/RCNN Dataset'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNhA20RNOK9I"
      },
      "source": [
        "class Dataset:\r\n",
        "  def __init__(self, root, train = True):\r\n",
        "    self.root = root\r\n",
        "    self.train = train\r\n",
        "\r\n",
        "    self.mask_path = os.path.join(root, 'Labels/labels_json.json')\r\n",
        "    self.masks = self.parse_annotations(self.mask_path)\r\n",
        "\r\n",
        "    self.img_path = os.path.join(root, \"Images\")\r\n",
        "    self.file_names = list(self.masks.keys())\r\n",
        "    self.images = self.load_images()\r\n",
        "     \r\n",
        "  def load_images(self):\r\n",
        "    images = {}\r\n",
        "    for f in self.file_names:\r\n",
        "      images[f] = Image.open(os.path.join(self.img_path, f))\r\n",
        "    return images\r\n",
        "\r\n",
        "  def transform(self):\r\n",
        "    operations = [T.ToTensor()]\r\n",
        "    if self.train == True:\r\n",
        "      if np.random.random() < 0.5:\r\n",
        "        flip = T.RandomHorizontalFlip(p=0.9999)\r\n",
        "        operations.append(flip)\r\n",
        "      deg = np.random.uniform(0, 360)\r\n",
        "      rot = T.RandomRotation([deg-0.0001, deg+0.0001])\r\n",
        "      operations.append(rot)\r\n",
        "    return T.Compose(operations)\r\n",
        "\r\n",
        "  def clean_coords(self, coords):\r\n",
        "    x = [int(i) for i in coords[::2]]\r\n",
        "    y = [int(i) for i in coords[1::2]]\r\n",
        "    return [x,y]\r\n",
        "\r\n",
        "  def create_mask(self, file_name):\r\n",
        "    img = self.images[file_name]\r\n",
        "    coords = self.masks[file_name]\r\n",
        "    poly = Image.new('RGBA', img.size)\r\n",
        "    draw = ImageDraw.Draw(poly)\r\n",
        "    draw.polygon(coords, fill = 'black')\r\n",
        "\r\n",
        "    x,y = self.clean_coords(self.masks[file_name])\r\n",
        "    box =  [min(x), min(y), max(x), max(y)]\r\n",
        "    return poly, box\r\n",
        "\r\n",
        "  def apply_mask(self, file_name):\r\n",
        "    transform = self.transform()\r\n",
        "    img = T.ToPILImage(mode='RGB')(transform(self.images[file_name]))\r\n",
        "    mask, _ = self.create_mask(file_name)\r\n",
        "    transformed_mask = T.ToPILImage(mode='RGBA')(transform(mask))\r\n",
        "    new_image = Image.new('RGBA', img.size)\r\n",
        "    new_image.paste(img, (0,0), mask = mask)\r\n",
        "    return new_image.convert('RGB')\r\n",
        "\r\n",
        "  def parse_annotations(self, mask_path):\r\n",
        "    via_json = json.load(open(mask_path, 'r'))\r\n",
        "    file_map = {key: {'fname':value['fname']} for key,value in via_json['file'].items()}\r\n",
        "    img_mask = {}\r\n",
        "    for value in via_json['metadata'].values():\r\n",
        "      # if coords not empty and is polygon (7)\r\n",
        "      coords = value['xy']\r\n",
        "      if coords != [] and coords[0] == 7:\r\n",
        "        file_name = file_map[value['vid']]['fname']\r\n",
        "        img_mask[file_name] = list(map(int,coords[1:]))\r\n",
        "    return img_mask\r\n",
        "\r\n",
        "class DataSetLoader(Dataset):\r\n",
        "  def __init__(self, root, train = True):\r\n",
        "    super().__init__(root) \r\n",
        "    self.train = train\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.file_names)\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    transform = self.transform()\r\n",
        "    # loading image and mask\r\n",
        "    file_name = self.file_names[idx]\r\n",
        "    img = transform(self.images[file_name])\r\n",
        "    raw_mask, box = self.create_mask(file_name)\r\n",
        "    mask = transform(raw_mask)\r\n",
        "\r\n",
        "    # vectorization \r\n",
        "    mask = np.array(mask)\r\n",
        "    obj_ids = np.unique(mask)[1:]\r\n",
        "    num_objs = len(obj_ids)\r\n",
        "    masks = mask == obj_ids[:, None, None]\r\n",
        "\r\n",
        "    # calculating bounding box\r\n",
        "    pos = np.where(mask)\r\n",
        "    boxes = torch.as_tensor([box], dtype=torch.float32)\r\n",
        "    area = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])\r\n",
        "    \r\n",
        "    # torch conversions\r\n",
        "    image_id = torch.tensor([idx])\r\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\r\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\r\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n",
        "    target = {'boxes': boxes, 'labels': labels, \r\n",
        "              'masks': masks, ' image_id': image_id, \r\n",
        "              'area': area, 'iscrowd' : iscrowd}\r\n",
        "    return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIRixddTtqn4"
      },
      "source": [
        "os.chdir(os.path.join(root, 'ExtraTools')) \r\n",
        "# https://github.com/pytorch/vision/tree/master/references/detection\r\n",
        "from utils import * \r\n",
        "from engine import *\r\n",
        "\r\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\r\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\r\n",
        "\r\n",
        "def get_model_instance_segmentation(num_classes):\r\n",
        "  model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\r\n",
        "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\r\n",
        "  hidden_layer = 256\r\n",
        "  model.roi_heads.mask_predictor = MaskRCNNPredictor(\r\n",
        "      in_features_mask, hidden_layer,num_classes)\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB9HfLucsug2"
      },
      "source": [
        "# object or background\r\n",
        "num_classes = 2\r\n",
        "\r\n",
        "# splitting data\r\n",
        "train = DataSetLoader(root, train = True)\r\n",
        "test = DataSetLoader(root, train = False)\r\n",
        "indices = torch.randperm(len(train)).tolist()\r\n",
        "train = torch.utils.data.Subset(train, indices[:-50])\r\n",
        "test = torch.utils.data.Subset(test, indices[-50:])\r\n",
        "\r\n",
        "# loader init \r\n",
        "train_loader = DataLoader(train, batch_size=2, \r\n",
        "                         shuffle=True, num_workers=4,\r\n",
        "                         collate_fn= collate_fn)\r\n",
        "test_loader = DataLoader(test, batch_size=1, \r\n",
        "                         shuffle=True, num_workers=4,\r\n",
        "                         collate_fn= collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcRhWg-Fsusk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "a2760aeb955c43209a670fc078cf9d3b",
            "d336beccf91940c7a3602a146b7cbeb2",
            "99c694a6654247f4881e546a9d3a6b17",
            "48af9dd7dfd84fb8a9312baf69184f33",
            "4c5cb0421cb546c49ff0d9cc3072ef59",
            "ca4e9cd872ff4ff591845ea3223bcefb",
            "ddc43de903534da4a0f217955fbc53b8",
            "8e8a351f314d4ea69a0a683467ba1a28"
          ]
        },
        "outputId": "ff4d6918-9047-4aca-ad12-7fe259b81b91"
      },
      "source": [
        "# model init\r\n",
        "\r\n",
        "device = torch.device('cuda') \r\n",
        "model = get_model_instance_segmentation(num_classes)\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "# optimizaer init\r\n",
        "params = [p for p in model.parameters() if p.requires_grad]\r\n",
        "optimizer = torch.optim.SGD(\r\n",
        "    params, lr=0.005, momentum=0.9, weight_decay=0.0005)\r\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\r\n",
        "    optimizer, step_size=3, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2760aeb955c43209a670fc078cf9d3b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Cua5Giu4S7"
      },
      "source": [
        "num_epochs = 10\r\n",
        "for epoch in range(num_epochs):\r\n",
        "  train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\r\n",
        "  lr_scheduler.step()"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}